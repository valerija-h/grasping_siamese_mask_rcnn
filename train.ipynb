{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Grasping Siamese Mask R-CNN - Training on the OCID Grasp Dataset**\n",
    "\n",
    "This notebook provides a demonstration on how to train the Grasping Siamese Mask R-CNN (GSMR-CNN) model on the __[OCID grasp](https://github.com/stefan-ainetter/grasp_det_seg_cnn)__ dataset. The OCID grasp dataset is an extension of the __[OCID](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/)__ dataset which was designed for robotic vision tasks that includes object-segmentation, classification and recognition. It contains RGBD scenes of cluttered objects where each objects is annotated with a class and segmentation map. The grasping extension adds multiple hand-annotated grasp candidates for each objects.\n",
    "\n",
    "The code below adds the python modules (i.e. `model.py`, `utils.py`) needed to build the dataloader and construct the GSMR-CNN model. The code used to build GSMR-CNN overwrites methods that were used to build both __[Siamese Mask R-CNN](https://github.com/bethgelab/siamese-mask-rcnn)__ and __[Mask R-CNN](https://github.com/matterport/Mask_RCNN)__, hence, both libraries need to be included in the project directory. This snippet also instantiates a configuration object, which uses the default variables specified in `config.py`. \n",
    "\n",
    "* Before running this folder, make sure to set the variables `DATASET_PATH` and `ANNOTATIONS_PATH` to the directory paths of the OCID grasp dataset and the folder where the COCO-formatted annotations of the OCID grasp dataset were placed. Note that if you do not have the annotations, it is important you ran the `generate_COCO.py` file to generate COCO-formatted annotations before running this code as specified in the `ReadMe` file. Make sure you also downloaded the ImageNet weights into the `models/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ignore FutureWarnings and Tensorflow warnings\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "\n",
    "# add paths to all libraries\n",
    "import sys\n",
    "sys.path.append('libraries/')\n",
    "sys.path.append('libraries/Mask_RCNN') \n",
    "sys.path.append('libraries/Siamese_Mask_RCNN') \n",
    "sys.path.append('gsmrcnn/')\n",
    "\n",
    "import model as gsmrcnn_model\n",
    "import config as gsmrcnn_config\n",
    "import utils as gsmrcnn_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "config = gsmrcnn_config.OCIDConfig()\n",
    "\n",
    "DATASET_PATH = 'data/OCID_grasp/'\n",
    "ANNOTATIONS_PATH = 'data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**\n",
    "\n",
    "The code in this section loads and prepares the dataset for training. It also selects which object classes will be used for training and testing the model. Note that certain object classes were excluded from both training and testing set because they were missing grasp annotations mainly due to them being large objects (e.g. keyboard).\n",
    "\n",
    "* The code below is used to train the model on an **object-wise split**, however, to train it on an image-wise split the lines `train_classes = all_classes` and `test_classes = all_classes` should be uncommented and used instead. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.65s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "[INFO] Number of training images: 1411\n",
      "[INFO] Number of validation images: 352\n"
     ]
    }
   ],
   "source": [
    "ignore_classes = [5, 7, 11, 12, 16, 17] # exclude classes due to abundant missing grasps...\n",
    "\n",
    "# for object-wise split\n",
    "all_classes = np.array([i for i in range(1,32) if i not in ignore_classes])\n",
    "train_classes, test_classes = train_test_split(all_classes, test_size=0.30, random_state=10)\n",
    "\n",
    "# for image-wise split (uncomment lines below if needed)\n",
    "# train_classes = all_classes\n",
    "# test_classes = all_classes\n",
    "\n",
    "# load training dataset\n",
    "coco_train = gsmrcnn_utils.OCIDDataset()\n",
    "coco_train.load_coco(os.path.join(ANNOTATIONS_PATH, 'training_annotations.json'), DATASET_PATH)\n",
    "coco_train.prepare()\n",
    "coco_train.build_indices() # creates lists mapping object classes ids to image ids and vice versa\n",
    "coco_train.ACTIVE_CLASSES = train_classes\n",
    "\n",
    "# load validation dataset\n",
    "coco_val = gsmrcnn_utils.OCIDDataset()\n",
    "coco_val.load_coco(os.path.join(ANNOTATIONS_PATH, 'validation_annotations.json'), DATASET_PATH)\n",
    "coco_val.prepare()\n",
    "coco_val.build_indices() # creates lists mapping object classes ids to image ids and vice versa\n",
    "coco_val.ACTIVE_CLASSES = test_classes\n",
    "\n",
    "print(\"[INFO] Number of training images:\", len(coco_train.image_category_index))\n",
    "print(\"[INFO] Number of validation images:\", len(coco_val.image_category_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**\n",
    "\n",
    "The code below is used to construct the GSMR-CNN model with pre-trained weights from ImageNet. The model is trained for 25 epochs and a scheduler is used to train the head components of the model for the first epoch and the entire model for the remaining epochs, having a lower learning rate for the last five epochs. \n",
    "\n",
    "* Note that it will create a directory in `{cfg.CHECKPOINT_DIR}/gsmrcnn_{cfg.NAME}_{cfg.EXPERIMENT}` to store training checkpoints under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing from imagenet weights ...\n"
     ]
    }
   ],
   "source": [
    "# create model and load weights trained on Imagenet\n",
    "model = gsmrcnn_model.GraspingSiameseMaskRCNN(mode=\"training\", model_dir=config.CHECKPOINT_DIR, config=config)\n",
    "model.load_imagenet_weights(pretraining='imagenet-687')\n",
    "\n",
    "# create a scheduler\n",
    "train_schedule = OrderedDict()\n",
    "train_schedule[1] = {\"learning_rate\": config.LEARNING_RATE, \"layers\": \"heads\"}\n",
    "train_schedule[20] = {\"learning_rate\": config.LEARNING_RATE, \"layers\": \"all\"}\n",
    "train_schedule[25] = {\"learning_rate\": config.LEARNING_RATE / 10, \"layers\": \"all\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**\n",
    "\n",
    "The code below can be run to train the network. \n",
    "\n",
    "* Note that the saved model will be saved as `{cfg.CHECKPOINT_DIR}/gsmrcnn_{cfg.NAME}_{cfg.EXPERIMENT}/gsmrcnn_0025.h5`. Additionally, the output below is output obtained from the terminal when training one of the models during experimentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layers heads until epoch 1 with learning_rate 0.02\n",
      "\n",
      "Starting at epoch 0. LR=0.02\n",
      "\n",
      "Checkpoint Path: models/gsmrcnn_object_split_1\\gsmrcnn_{epoch:04d}.h5\n",
      "Epoch 1/1\n",
      "  150/150 [==============================] - 2070s 14s/step - loss: 1.7335 - rpn_class_loss: 0.3302 - rpn_bbox_loss: 0.1097 - mrcnn_class_loss: 0.2470 - mrcnn_bbox_loss: 0.3087 - mrcnn_mask_loss: 0.5859 - mrcnn_grasp_loss: 0.1519 - val_loss: 1.1943 - val_rpn_class_loss: 0.1087 - val_rpn_bbox_loss: 0.0720 - val_mrcnn_class_loss: 0.2554 - val_mrcnn_bbox_loss: 0.2312 - val_mrcnn_mask_loss: 0.4575 - val_mrcnn_grasp_loss: 0.0694\n",
      "training layers all until epoch 20 with learning_rate 0.02\n",
      "\n",
      "Checkpoint Path: models/gsmrcnn_object_split_1\\gsmrcnn_{epoch:04d}.h5\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 2357s 16s/step - loss: 1.1893 - rpn_class_loss: 0.0958 - rpn_bbox_loss: 0.0614 - mrcnn_class_loss: 0.3289 - mrcnn_bbox_loss: 0.2038 - mrcnn_mask_loss: 0.4433 - mrcnn_grasp_loss: 0.0558 - val_loss: 0.9978 - val_rpn_class_loss: 0.0729 - val_rpn_bbox_loss: 0.0603 - val_mrcnn_class_loss: 0.2512 - val_mrcnn_bbox_loss: 0.1739 - val_mrcnn_mask_loss: 0.4012 - val_mrcnn_grasp_loss: 0.0380\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 2341s 16s/step - loss: 0.9844 - rpn_class_loss: 0.0899 - rpn_bbox_loss: 0.0563 - mrcnn_class_loss: 0.2962 - mrcnn_bbox_loss: 0.1475 - mrcnn_mask_loss: 0.3486 - mrcnn_grasp_loss: 0.0456 - val_loss: 0.6861 - val_rpn_class_loss: 0.0564 - val_rpn_bbox_loss: 0.0392 - val_mrcnn_class_loss: 0.1625 - val_mrcnn_bbox_loss: 0.0890 - val_mrcnn_mask_loss: 0.3035 - val_mrcnn_grasp_loss: 0.0353\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 2340s 16s/step - loss: 0.7745 - rpn_class_loss: 0.0702 - rpn_bbox_loss: 0.0447 - mrcnn_class_loss: 0.2309 - mrcnn_bbox_loss: 0.1152 - mrcnn_mask_loss: 0.2785 - mrcnn_grasp_loss: 0.0348 - val_loss: 0.5726 - val_rpn_class_loss: 0.0394 - val_rpn_bbox_loss: 0.0399 - val_mrcnn_class_loss: 0.1261 - val_mrcnn_bbox_loss: 0.0996 - val_mrcnn_mask_loss: 0.2346 - val_mrcnn_grasp_loss: 0.0327\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 2337s 16s/step - loss: 0.6675 - rpn_class_loss: 0.0434 - rpn_bbox_loss: 0.0412 - mrcnn_class_loss: 0.1835 - mrcnn_bbox_loss: 0.1079 - mrcnn_mask_loss: 0.2575 - mrcnn_grasp_loss: 0.0338 - val_loss: 0.7682 - val_rpn_class_loss: 0.0481 - val_rpn_bbox_loss: 0.0359 - val_mrcnn_class_loss: 0.2699 - val_mrcnn_bbox_loss: 0.1230 - val_mrcnn_mask_loss: 0.2519 - val_mrcnn_grasp_loss: 0.0391\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 2343s 16s/step - loss: 0.6288 - rpn_class_loss: 0.0442 - rpn_bbox_loss: 0.0387 - mrcnn_class_loss: 0.1826 - mrcnn_bbox_loss: 0.0950 - mrcnn_mask_loss: 0.2376 - mrcnn_grasp_loss: 0.0305 - val_loss: 0.6540 - val_rpn_class_loss: 0.0571 - val_rpn_bbox_loss: 0.0429 - val_mrcnn_class_loss: 0.1987 - val_mrcnn_bbox_loss: 0.0961 - val_mrcnn_mask_loss: 0.2245 - val_mrcnn_grasp_loss: 0.0344\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 2346s 16s/step - loss: 0.5653 - rpn_class_loss: 0.0315 - rpn_bbox_loss: 0.0355 - mrcnn_class_loss: 0.1602 - mrcnn_bbox_loss: 0.0840 - mrcnn_mask_loss: 0.2241 - mrcnn_grasp_loss: 0.0297 - val_loss: 0.5528 - val_rpn_class_loss: 0.0264 - val_rpn_bbox_loss: 0.0200 - val_mrcnn_class_loss: 0.1824 - val_mrcnn_bbox_loss: 0.0848 - val_mrcnn_mask_loss: 0.2125 - val_mrcnn_grasp_loss: 0.0265\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 2355s 16s/step - loss: 0.4930 - rpn_class_loss: 0.0273 - rpn_bbox_loss: 0.0324 - mrcnn_class_loss: 0.1412 - mrcnn_bbox_loss: 0.0715 - mrcnn_mask_loss: 0.1916 - mrcnn_grasp_loss: 0.0288 - val_loss: 0.5343 - val_rpn_class_loss: 0.0333 - val_rpn_bbox_loss: 0.0305 - val_mrcnn_class_loss: 0.1302 - val_mrcnn_bbox_loss: 0.0794 - val_mrcnn_mask_loss: 0.2224 - val_mrcnn_grasp_loss: 0.0383\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 2363s 16s/step - loss: 0.4768 - rpn_class_loss: 0.0276 - rpn_bbox_loss: 0.0280 - mrcnn_class_loss: 0.1323 - mrcnn_bbox_loss: 0.0688 - mrcnn_mask_loss: 0.1918 - mrcnn_grasp_loss: 0.0281 - val_loss: 0.4543 - val_rpn_class_loss: 0.0156 - val_rpn_bbox_loss: 0.0183 - val_mrcnn_class_loss: 0.1586 - val_mrcnn_bbox_loss: 0.0625 - val_mrcnn_mask_loss: 0.1725 - val_mrcnn_grasp_loss: 0.0264\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 2362s 16s/step - loss: 0.4261 - rpn_class_loss: 0.0220 - rpn_bbox_loss: 0.0232 - mrcnn_class_loss: 0.1110 - mrcnn_bbox_loss: 0.0636 - mrcnn_mask_loss: 0.1782 - mrcnn_grasp_loss: 0.0278 - val_loss: 0.3177 - val_rpn_class_loss: 0.0116 - val_rpn_bbox_loss: 0.0335 - val_mrcnn_class_loss: 0.0331 - val_mrcnn_bbox_loss: 0.0535 - val_mrcnn_mask_loss: 0.1653 - val_mrcnn_grasp_loss: 0.0203\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 2364s 16s/step - loss: 0.3853 - rpn_class_loss: 0.0144 - rpn_bbox_loss: 0.0223 - mrcnn_class_loss: 0.1086 - mrcnn_bbox_loss: 0.0529 - mrcnn_mask_loss: 0.1642 - mrcnn_grasp_loss: 0.0227 - val_loss: 0.3140 - val_rpn_class_loss: 0.0112 - val_rpn_bbox_loss: 0.0256 - val_mrcnn_class_loss: 0.0558 - val_mrcnn_bbox_loss: 0.0462 - val_mrcnn_mask_loss: 0.1539 - val_mrcnn_grasp_loss: 0.0211\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 2361s 16s/step - loss: 0.3731 - rpn_class_loss: 0.0159 - rpn_bbox_loss: 0.0197 - mrcnn_class_loss: 0.0978 - mrcnn_bbox_loss: 0.0523 - mrcnn_mask_loss: 0.1664 - mrcnn_grasp_loss: 0.0208 - val_loss: 0.4174 - val_rpn_class_loss: 0.0307 - val_rpn_bbox_loss: 0.0254 - val_mrcnn_class_loss: 0.1340 - val_mrcnn_bbox_loss: 0.0510 - val_mrcnn_mask_loss: 0.1563 - val_mrcnn_grasp_loss: 0.0197\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 2363s 16s/step - loss: 0.3779 - rpn_class_loss: 0.0197 - rpn_bbox_loss: 0.0230 - mrcnn_class_loss: 0.1040 - mrcnn_bbox_loss: 0.0526 - mrcnn_mask_loss: 0.1582 - mrcnn_grasp_loss: 0.0201 - val_loss: 0.3552 - val_rpn_class_loss: 0.0131 - val_rpn_bbox_loss: 0.0223 - val_mrcnn_class_loss: 0.0819 - val_mrcnn_bbox_loss: 0.0504 - val_mrcnn_mask_loss: 0.1642 - val_mrcnn_grasp_loss: 0.0230\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 2362s 16s/step - loss: 0.3174 - rpn_class_loss: 0.0113 - rpn_bbox_loss: 0.0189 - mrcnn_class_loss: 0.0715 - mrcnn_bbox_loss: 0.0444 - mrcnn_mask_loss: 0.1516 - mrcnn_grasp_loss: 0.0195 - val_loss: 0.2536 - val_rpn_class_loss: 0.0035 - val_rpn_bbox_loss: 0.0240 - val_mrcnn_class_loss: 0.0429 - val_mrcnn_bbox_loss: 0.0327 - val_mrcnn_mask_loss: 0.1280 - val_mrcnn_grasp_loss: 0.0221\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 2360s 16s/step - loss: 0.3369 - rpn_class_loss: 0.0149 - rpn_bbox_loss: 0.0193 - mrcnn_class_loss: 0.0898 - mrcnn_bbox_loss: 0.0453 - mrcnn_mask_loss: 0.1488 - mrcnn_grasp_loss: 0.0186 - val_loss: 0.3280 - val_rpn_class_loss: 0.0106 - val_rpn_bbox_loss: 0.0156 - val_mrcnn_class_loss: 0.1056 - val_mrcnn_bbox_loss: 0.0362 - val_mrcnn_mask_loss: 0.1475 - val_mrcnn_grasp_loss: 0.0122\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 2363s 16s/step - loss: 0.3173 - rpn_class_loss: 0.0153 - rpn_bbox_loss: 0.0164 - mrcnn_class_loss: 0.0855 - mrcnn_bbox_loss: 0.0407 - mrcnn_mask_loss: 0.1388 - mrcnn_grasp_loss: 0.0204 - val_loss: 0.2712 - val_rpn_class_loss: 0.0047 - val_rpn_bbox_loss: 0.0245 - val_mrcnn_class_loss: 0.0477 - val_mrcnn_bbox_loss: 0.0418 - val_mrcnn_mask_loss: 0.1382 - val_mrcnn_grasp_loss: 0.0139\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 2361s 16s/step - loss: 0.2838 - rpn_class_loss: 0.0093 - rpn_bbox_loss: 0.0150 - mrcnn_class_loss: 0.0694 - mrcnn_bbox_loss: 0.0365 - mrcnn_mask_loss: 0.1351 - mrcnn_grasp_loss: 0.0181 - val_loss: 0.2712 - val_rpn_class_loss: 0.0154 - val_rpn_bbox_loss: 0.0144 - val_mrcnn_class_loss: 0.0686 - val_mrcnn_bbox_loss: 0.0324 - val_mrcnn_mask_loss: 0.1207 - val_mrcnn_grasp_loss: 0.0195\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 2360s 16s/step - loss: 0.2965 - rpn_class_loss: 0.0113 - rpn_bbox_loss: 0.0145 - mrcnn_class_loss: 0.0794 - mrcnn_bbox_loss: 0.0374 - mrcnn_mask_loss: 0.1370 - mrcnn_grasp_loss: 0.0167 - val_loss: 0.2580 - val_rpn_class_loss: 0.0040 - val_rpn_bbox_loss: 0.0135 - val_mrcnn_class_loss: 0.0675 - val_mrcnn_bbox_loss: 0.0303 - val_mrcnn_mask_loss: 0.1214 - val_mrcnn_grasp_loss: 0.0210\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 2366s 16s/step - loss: 0.2822 - rpn_class_loss: 0.0093 - rpn_bbox_loss: 0.0138 - mrcnn_class_loss: 0.0745 - mrcnn_bbox_loss: 0.0329 - mrcnn_mask_loss: 0.1333 - mrcnn_grasp_loss: 0.0181 - val_loss: 0.3588 - val_rpn_class_loss: 0.0105 - val_rpn_bbox_loss: 0.0176 - val_mrcnn_class_loss: 0.0807 - val_mrcnn_bbox_loss: 0.0528 - val_mrcnn_mask_loss: 0.1697 - val_mrcnn_grasp_loss: 0.0272\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 2365s 16s/step - loss: 0.2684 - rpn_class_loss: 0.0109 - rpn_bbox_loss: 0.0140 - mrcnn_class_loss: 0.0651 - mrcnn_bbox_loss: 0.0329 - mrcnn_mask_loss: 0.1274 - mrcnn_grasp_loss: 0.0178 - val_loss: 0.2557 - val_rpn_class_loss: 0.0077 - val_rpn_bbox_loss: 0.0157 - val_mrcnn_class_loss: 0.0546 - val_mrcnn_bbox_loss: 0.0306 - val_mrcnn_mask_loss: 0.1286 - val_mrcnn_grasp_loss: 0.0183\n",
      "training layers all until epoch 25 with learning_rate 0.002\n",
      "\n",
      "Starting at epoch 20. LR=0.002\n",
      "\n",
      "Checkpoint Path: models/gsmrcnn_object_split_1\\gsmrcnn_{epoch:04d}.h5\n",
      "Epoch 21/25\n",
      "150/150 [==============================] - 2363s 16s/step - loss: 0.2143 - rpn_class_loss: 0.0077 - rpn_bbox_loss: 0.0107 - mrcnn_class_loss: 0.0420 - mrcnn_bbox_loss: 0.0246 - mrcnn_mask_loss: 0.1159 - mrcnn_grasp_loss: 0.0131 - val_loss: 0.2566 - val_rpn_class_loss: 0.0084 - val_rpn_bbox_loss: 0.0118 - val_mrcnn_class_loss: 0.0532 - val_mrcnn_bbox_loss: 0.0311 - val_mrcnn_mask_loss: 0.1357 - val_mrcnn_grasp_loss: 0.0160\n",
      "Epoch 22/25\n",
      "150/150 [==============================] - 2361s 16s/step - loss: 0.2058 - rpn_class_loss: 0.0075 - rpn_bbox_loss: 0.0093 - mrcnn_class_loss: 0.0378 - mrcnn_bbox_loss: 0.0233 - mrcnn_mask_loss: 0.1157 - mrcnn_grasp_loss: 0.0120 - val_loss: 0.1821 - val_rpn_class_loss: 0.0023 - val_rpn_bbox_loss: 0.0082 - val_mrcnn_class_loss: 0.0256 - val_mrcnn_bbox_loss: 0.0208 - val_mrcnn_mask_loss: 0.1139 - val_mrcnn_grasp_loss: 0.0110\n",
      "Epoch 23/25\n",
      "150/150 [==============================] - 2370s 16s/step - loss: 0.1896 - rpn_class_loss: 0.0056 - rpn_bbox_loss: 0.0085 - mrcnn_class_loss: 0.0344 - mrcnn_bbox_loss: 0.0203 - mrcnn_mask_loss: 0.1065 - mrcnn_grasp_loss: 0.0141 - val_loss: 0.2698 - val_rpn_class_loss: 0.0070 - val_rpn_bbox_loss: 0.0132 - val_mrcnn_class_loss: 0.0393 - val_mrcnn_bbox_loss: 0.0374 - val_mrcnn_mask_loss: 0.1555 - val_mrcnn_grasp_loss: 0.0171\n",
      "Epoch 24/25\n",
      "150/150 [==============================] - 2369s 16s/step - loss: 0.1919 - rpn_class_loss: 0.0040 - rpn_bbox_loss: 0.0078 - mrcnn_class_loss: 0.0390 - mrcnn_bbox_loss: 0.0193 - mrcnn_mask_loss: 0.1063 - mrcnn_grasp_loss: 0.0153 - val_loss: 0.1870 - val_rpn_class_loss: 0.0063 - val_rpn_bbox_loss: 0.0099 - val_mrcnn_class_loss: 0.0192 - val_mrcnn_bbox_loss: 0.0182 - val_mrcnn_mask_loss: 0.1106 - val_mrcnn_grasp_loss: 0.0226\n",
      "Epoch 25/25\n",
      "150/150 [==============================] - 2368s 16s/step - loss: 0.1866 - rpn_class_loss: 0.0061 - rpn_bbox_loss: 0.0078 - mrcnn_class_loss: 0.0358 - mrcnn_bbox_loss: 0.0187 - mrcnn_mask_loss: 0.1054 - mrcnn_grasp_loss: 0.0125 - val_loss: 0.1678 - val_rpn_class_loss: 0.0024 - val_rpn_bbox_loss: 0.0076 - val_mrcnn_class_loss: 0.0340 - val_mrcnn_bbox_loss: 0.0182 - val_mrcnn_mask_loss: 0.0975 - val_mrcnn_grasp_loss: 0.0079"
     ]
    }
   ],
   "source": [
    "for epochs, parameters in train_schedule.items():\n",
    "    print(\"training layers {} until epoch {} with learning_rate {}\".format(parameters[\"layers\"], epochs, parameters[\"learning_rate\"]))\n",
    "    model.train(coco_train, coco_val,\n",
    "                learning_rate=parameters[\"learning_rate\"],\n",
    "                epochs=epochs,\n",
    "                layers=parameters[\"layers\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsmrcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63ab86ac6c55da7d34b6f19d322018044dfea61473581a7913dd545d3680e91f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
