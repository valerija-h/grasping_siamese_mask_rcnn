import sys
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # ignore warnings
import tensorflow as tf
tf.get_logger().setLevel('INFO')
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
import numpy as np
import os
import keras
import keras.backend as K
import keras.layers as KL
import keras.engine as KE
import keras.models as KM
import multiprocessing

# add the path to both libraries (Mask R-CNN and Siamese Mask R-CNN)
sys.path.append('../libraries/')
sys.path.append('../libraries/Mask_RCNN') # add path to Mask R-CNN library
sys.path.append('../libraries/Siamese_Mask_RCNN') # add path to Mask R-CNN library
from Siamese_Mask_RCNN.lib import utils as smrcnn_utils
from Mask_RCNN.mrcnn import model as mrcnn_model
from Mask_RCNN.mrcnn import utils as mrcnn_utils
from Siamese_Mask_RCNN.lib import model as smrcnn_model

import utils as gsmrcnn_utils

''' This file contains classes and functions to create and train the GSM-RCNN model. '''

############################################################
#  Data Generator
############################################################

def load_image_gt(dataset, config, image_id, augment=False, augmentation=None,
                  use_mini_mask=False):
    """Load and return ground truth data for an image (image, mask, bounding boxes, grasps).

    use_mini_mask: If False, returns full-size masks that are the same height
        and width as the original image. These can be big, for example
        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,
        224x224 and are generated by extracting the bounding box of the
        object and resizing it to MINI_MASK_SHAPE.

    Returns:
    image: [height, width, 3]
    shape: the original shape of the image before resizing and cropping.
    class_ids: [instance_count] Integer class IDs
    bbox: [instance_count, (y1, x1, y2, x2)]
    mask: [height, width, instance_count]. The height and width are those
        of the image unless use_mini_mask is True, in which case they are
        defined in MINI_MASK_SHAPE.
    grasps: [instance_count, MAX_GT_GRASPS, (x, y, w, h, t)], where x,y are
            the centre co-ordinates of the grasp, and w,h are the width and
            height of the grasp rectangle and t is the rotation given in the
            range [-pi/2, pi/2].

    CHANGE: The grasps are only resized properly if image is ONLY padded for
            now. Other augmentation will be supported later. To ensure this,
            make sure the config.IMAGE_RESIZE_MODE == 'square' and the 
            config.IMAGE_MAX_DIM == *width of your image*.
    CHANGE: Augmentation is not supported for now.
    """
    # Load image and mask
    image = dataset.load_image(image_id)
    mask, class_ids = dataset.load_mask(image_id)
    # CHANGE: Load grasps
    grasps = dataset.load_grasps(image_id, config.MAX_GT_GRASPS)
    # Resize, crop, scale image and mask
    original_shape = image.shape
    image, window, scale, padding, crop = mrcnn_utils.resize_image(
        image,
        min_dim=config.IMAGE_MIN_DIM,
        min_scale=config.IMAGE_MIN_SCALE,
        max_dim=config.IMAGE_MAX_DIM,
        mode=config.IMAGE_RESIZE_MODE)
    # CHANGE: Adjust grasp values
    grasps = resize_grasps(grasps, padding) # only supports padding option for now
    mask = mrcnn_utils.resize_mask(mask, scale, padding, crop)

    # CHANGE: Removed augmentation

    # Removes masks and bboxes that have been cropped out
    _idx = np.sum(mask, axis=(0, 1)) > 0
    mask = mask[:, :, _idx]
    class_ids = class_ids[_idx]
    bbox = mrcnn_utils.extract_bboxes(mask)

    # Active classes
    # Different datasets have different classes, so track the classes supported in the dataset of this image.
    active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)
    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id]["source"]]
    active_class_ids[source_class_ids] = 1

    # Resize masks to smaller size to reduce memory usage
    if use_mini_mask:
        mask = mrcnn_utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)

    # Image meta data
    image_meta = mrcnn_model.compose_image_meta(image_id, original_shape, image.shape,
                                    window, scale, active_class_ids)

    return image, image_meta, class_ids, bbox, mask, grasps


# CHANGE - new data generator function that also generates ground truth grasps
def grasp_data_generator(dataset, config, shuffle=True, augmentation=None, random_rois=0, batch_size=1, detection_targets=False):
    """ A generator that returns images and corresponding target class ids,
    bounding box deltas, and masks.

    dataset: The Dataset object to pick data from
    config: The model config object
    shuffle: If True, shuffles the samples before every epoch
    augmentation: Augmentation is not currently supported for GSM-RCNN.
    random_rois: If > 0 then generate proposals to be used to train the
                 network classifier and mask heads. Useful if training
                 the Mask RCNN part without the RPN.
    batch_size: How many images to return in each call
    detection_targets: If True, generate detection targets (class IDs, bbox
        deltas, and masks). Typically for debugging or visualizations because
        in trainig detection targets are generated by DetectionTargetLayer.

    Returns a Python generator. Upon calling next() on it, the
    generator returns two lists, inputs and outputs. The contents
    of the lists differs depending on the received arguments:
    inputs list:
    - images: [batch, H, W, C]
    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()
    - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)
    - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs
    - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]
    - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width
                are those of the image unless use_mini_mask is True, in which
                case they are defined in MINI_MASK_SHAPE.
    - gt_grasps: [batch, MAX_GT_INSTANCES, MAX_GT_GRASPS, (cx, cy, w, h, t)]

    outputs list: Usually empty in regular training. But if detection_targets
        is True then the outputs list contains target class_ids, bbox deltas,
        and masks.
    """
    b = 0  # batch item idx
    image_index = -1
    image_ids = np.copy(dataset.image_ids)
    error_count = 0

    # Generate anchors [anchor_count, (y1, x1, y2, x2)]
    backbone_shapes = mrcnn_model.compute_backbone_shapes(config, config.IMAGE_SHAPE)
    anchors = mrcnn_utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,
                                             config.RPN_ANCHOR_RATIOS,
                                             backbone_shapes,
                                             config.BACKBONE_STRIDES,
                                             config.RPN_ANCHOR_STRIDE)

    # Keras requires a generator to run indefinitely.
    while True:
        try:
            # Increment index to pick next image. Shuffle if at the start of an epoch.
            image_index = (image_index + 1) % len(image_ids)
            if shuffle and image_index == 0:
                np.random.shuffle(image_ids)

            # CHANGE: Get GT bounding boxes, grasps and masks for current image 
            image_id = image_ids[image_index] 
            image, image_meta, gt_class_ids, gt_boxes, gt_masks, gt_grasps = \
                load_image_gt(dataset, config, image_id, augmentation=augmentation, use_mini_mask=config.USE_MINI_MASK)

            # Skip images that have no instances
            if not np.any(gt_class_ids > 0):
                continue

            # Get all non-background and ACTIVE instance class ids from the current image
            categories = np.unique(gt_class_ids)
            _idx = categories > 0
            categories = categories[_idx]
            active_categories = []
            for c in categories:
                if any(c == dataset.ACTIVE_CLASSES):
                    active_categories.append(c)
            
            # Skip images that have no ACTIVE class instances
            if not np.any(np.array(active_categories) > 0):
                continue

            # Select a random class ID (for the reference/anchor image)
            target_class_id = np.random.choice(active_categories)
            # Get reference image(s)
            targets = []
            for i in range(config.NUM_TARGETS):
                targets.append(gsmrcnn_utils.get_one_target(target_class_id, dataset, config, augmentation=augmentation,
                                                    image_id=image_id, apply_mask=True, custom_augmentation=True))

            # Filter all the instance annotations in the image to ones that have the target object class
            idx = gt_class_ids == target_class_id
            siamese_class_ids = idx.astype('int8')
            siamese_class_ids = siamese_class_ids[idx]
            gt_class_ids = gt_class_ids[idx]
            gt_boxes = gt_boxes[idx,:]
            gt_masks = gt_masks[:,:,idx]
            image_meta = image_meta[:14]
            # CHANGE: Filter relevant grasps
            gt_grasps = gt_grasps[idx]

            # CHANGE: Remove any invalid grasp ones too (i.e. if the instance in the image has no valid grasps...)
            # i.e. skip images which have no annotated grasps.
            missing_grasps = False
            for g in gt_grasps:
                if not np.any(g):
                    missing_grasps = True  # true if one of the target instances is missing grasps...
            if missing_grasps:
                continue

            # RPN Targets
            rpn_match, rpn_bbox = mrcnn_model.build_rpn_targets(image.shape, anchors, gt_class_ids, gt_boxes, config)

            # Mask R-CNN Targets
            if random_rois:
                rpn_rois = mrcnn_model.generate_random_rois(
                    image.shape, random_rois, gt_class_ids, gt_boxes)
                if detection_targets:
                    rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask =\
                        mrcnn_model.build_detection_targets(
                            rpn_rois, gt_class_ids, gt_boxes, gt_masks, config)

            # Init batch arrays
            if b == 0:
                batch_image_meta = np.zeros(
                    (batch_size,) + image_meta.shape, dtype=image_meta.dtype)
                batch_rpn_match = np.zeros(
                    [batch_size, anchors.shape[0], 1], dtype=rpn_match.dtype)
                batch_rpn_bbox = np.zeros(
                    [batch_size, config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)
                batch_images = np.zeros(
                    (batch_size,) + image.shape, dtype=np.float32)
                batch_gt_class_ids = np.zeros(
                    (batch_size, config.MAX_GT_INSTANCES), dtype=np.int32)
                batch_gt_boxes = np.zeros(
                    (batch_size, config.MAX_GT_INSTANCES, 4), dtype=np.int32)
                batch_targets = np.zeros(
                    (batch_size, config.NUM_TARGETS) + targets[0].shape, dtype=np.float32)
                # CHANGE: Create batch holder for ground truth grasps
                batch_gt_grasps = np.zeros(
                    (batch_size, config.MAX_GT_INSTANCES, config.MAX_GT_GRASPS, 5), dtype=np.float32)

                if config.USE_MINI_MASK:
                    batch_gt_masks = np.zeros((batch_size, config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1],
                                               config.MAX_GT_INSTANCES))
                else:
                    batch_gt_masks = np.zeros(
                        (batch_size, image.shape[0], image.shape[1], config.MAX_GT_INSTANCES))
                if random_rois:
                    batch_rpn_rois = np.zeros(
                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)
                    if detection_targets:
                        batch_rois = np.zeros(
                            (batch_size,) + rois.shape, dtype=rois.dtype)
                        batch_mrcnn_class_ids = np.zeros(
                            (batch_size,) + mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)
                        batch_mrcnn_bbox = np.zeros(
                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)
                        batch_mrcnn_mask = np.zeros(
                            (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)
            
            # If more instances than fits in the array, sub-sample from them.
            if gt_boxes.shape[0] > config.MAX_GT_INSTANCES:
                ids = np.random.choice(
                    np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)
                gt_class_ids = gt_class_ids[ids]
                siamese_class_ids = siamese_class_ids[ids]
                gt_boxes = gt_boxes[ids]
                gt_masks = gt_masks[:, :, ids]
                # CHANGE: Filter relevant grasps
                gt_grasps = gt_grasps[ids]

            # Add to batch
            batch_image_meta[b] = image_meta
            batch_rpn_match[b] = rpn_match[:, np.newaxis]
            batch_rpn_bbox[b] = rpn_bbox
            batch_images[b] = mrcnn_model.mold_image(image.astype(np.float32), config)
            batch_targets[b] = np.stack([mrcnn_model.mold_image(target.astype(np.float32), config) for target in targets], axis=0)
            batch_gt_class_ids[b, :siamese_class_ids.shape[0]] = siamese_class_ids
            batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes
            # CHANGE: Added ground-truth grasp batch loader
            batch_gt_grasps[b, :gt_grasps.shape[0]] = gt_grasps
            batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks
            if random_rois:
                batch_rpn_rois[b] = rpn_rois
                if detection_targets:
                    batch_rois[b] = rois
                    batch_mrcnn_class_ids[b] = mrcnn_class_ids
                    batch_mrcnn_bbox[b] = mrcnn_bbox
                    batch_mrcnn_mask[b] = mrcnn_mask
            b += 1

            # Batch full?
            if b >= batch_size:
                # CHANGE: batch_gt_grasps added as an input
                inputs = [batch_images, batch_image_meta, batch_targets, batch_rpn_match, batch_rpn_bbox,
                          batch_gt_class_ids, batch_gt_boxes, batch_gt_masks, batch_gt_grasps]
                outputs = []

                if random_rois:
                    inputs.extend([batch_rpn_rois])
                    if detection_targets:
                        inputs.extend([batch_rois])
                        # Keras requires that output and targets have the same number of dimensions
                        batch_mrcnn_class_ids = np.expand_dims(
                            batch_mrcnn_class_ids, -1)
                        outputs.extend(
                            [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])

                yield inputs, outputs

                # start a new batch
                b = 0
        except (GeneratorExit, KeyboardInterrupt):
            raise
        except:
            # Log it and skip the image
            mrcnn_model.logging.exception("Error processing image {}".format(
                dataset.image_info[image_id]))
            error_count += 1
            if error_count > 5:
                raise



############################################################
#  Model Class (GSM-RCNN)
############################################################

class GraspingSiameseMaskRCNN(smrcnn_model.SiameseMaskRCNN):
    """Encapsulates the Mask RCNN model functionality.
    The actual Keras model is in the keras_model property.
    """

    def build(self, mode, config):
        """Build Grasping Siamese Mask R-CNN architecture.
            mode: Either "training" or "inference". The inputs and
                outputs of the model differ accordingly.
        """
        assert mode in ['training', 'inference']

        # Check image size (must be dividable by 2 multiple times)
        h, w = config.IMAGE_SHAPE[:2]
        if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):
            raise Exception("Image size must be dividable by 2 at least 6 times "
                            "to avoid fractions when downscaling and upscaling."
                            "For example, use 256, 320, 384, 448, 512, ... etc. ")

        # Model input layers (image and target image)
        input_image = KL.Input(shape=config.IMAGE_SHAPE.tolist(), name="input_image")
        input_target = KL.Input(shape=[config.NUM_TARGETS] + config.TARGET_SHAPE.tolist(), name="input_target")
        input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE], name="input_image_meta")

        if mode == "training":
            # Add input layers for all ground truths passed into the model

            # RPN ground truths
            input_rpn_match = KL.Input(shape=[None, 1], name="input_rpn_match", dtype=tf.int32)
            input_rpn_bbox = KL.Input(shape=[None, 4], name="input_rpn_bbox", dtype=tf.float32)

            # Mask R-CNN heads ground truths (class IDs, bounding boxes, and masks [batch, height, width, MAX_GT_INSTANCES])
            input_gt_class_ids = KL.Input(shape=[None], name="input_gt_class_ids", dtype=tf.int32) # GT class IDs
            input_gt_boxes = KL.Input(shape=[None, 4], name="input_gt_boxes", dtype=tf.float32) # GT bounding boxes [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]
            # Normalize coordinates w.r.t to input image
            gt_boxes = KL.Lambda(lambda x: mrcnn_model.norm_boxes_graph(x, K.shape(input_image)[1:3]))(input_gt_boxes)

            # CHANGE: Add ground turth grasps and normalize them to image like above
            input_gt_grasps = KL.Input(shape=[None, config.MAX_GT_GRASPS, 5], name="input_gt_grasps", dtype=tf.float32)
            scale_gt_grasps = KL.Lambda(lambda x: norm_grasps_graph(x, K.shape(input_image)[1:3]))
            gt_grasps = KL.Lambda(lambda x: scale_gt_grasps(x))(input_gt_grasps)

            # GT masks [batch, height, width, MAX_GT_INSTANCES] (zero padded)
            if config.USE_MINI_MASK:
                input_gt_masks = KL.Input(shape=[config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1], None], name="input_gt_masks", dtype=bool)
            else:
                input_gt_masks = KL.Input(shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None], name="input_gt_masks", dtype=bool)
            
        elif mode == "inference":
            # Anchors in normalized coordinates
            input_anchors = KL.Input(shape=[None, 4], name="input_anchors")

        # Create the FPN Model (the weights will be shared for image and target/reference)
        resnet = smrcnn_model.build_resnet_model(self.config)
        fpn = smrcnn_model.build_fpn_model(feature_maps=self.config.FPN_FEATUREMAPS)

        # Create image features (i.e. features of observation image)
        _, IC2, IC3, IC4, IC5 = resnet(input_image) # bottom-up layers
        IP2, IP3, IP4, IP5, IP6 = fpn([IC2, IC3, IC4, IC5]) # top-down layers

        # Create target image features (i.e. features of reference image)
        input_targets = [KL.Lambda(lambda x: x[:,idx,...])(input_target) for idx in range(input_target.shape[1])]
        for k, one_target in enumerate(input_targets):
            _, TC2, TC3, TC4, TC5 = resnet(one_target)
            out = fpn([TC2, TC3, TC4, TC5])
            if k == 0:
                target_pyramid = out
            else:
                target_pyramid = [KL.Add(name="target_adding_{}_{}".format(k, i))(
                    [target_pyramid[i], out[i]]) for i in range(len(out))]
                
        TP2, TP3, TP4, TP5, TP6 = [KL.Lambda(lambda x: x / config.NUM_TARGETS)(target_pyramid[i]) for i in range(len(target_pyramid))]

        # Compute similarlity (i.e L1 distance) between image and target features
        P2 = smrcnn_model.l1_distance_graph(IP2, TP2, feature_maps = 3*self.config.FPN_FEATUREMAPS//2, name='P2')
        P3 = smrcnn_model.l1_distance_graph(IP3, TP3, feature_maps = 3*self.config.FPN_FEATUREMAPS//2, name='P3')
        P4 = smrcnn_model.l1_distance_graph(IP4, TP4, feature_maps = 3*self.config.FPN_FEATUREMAPS//2, name='P4')
        P5 = smrcnn_model.l1_distance_graph(IP5, TP5, feature_maps = 3*self.config.FPN_FEATUREMAPS//2, name='P5')
        P6 = smrcnn_model.l1_distance_graph(IP6, TP6, feature_maps = 3*self.config.FPN_FEATUREMAPS//2, name='P6')
        
        # Note that P6 is used in RPN, but not in the classifier heads.
        rpn_feature_maps = [P2, P3, P4, P5, P6]
        mrcnn_feature_maps = [P2, P3, P4, P5]

        # Anchors
        if mode == "training":
            anchors = self.get_anchors(config.IMAGE_SHAPE)
            anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)
            anchors = KL.Lambda(lambda x: tf.Variable(anchors), name="anchors")(input_image)
        else:
            anchors = input_anchors

        # Build RPN model
        rpn = mrcnn_model.build_rpn_model(config.RPN_ANCHOR_STRIDE, len(config.RPN_ANCHOR_RATIOS), 3*self.config.FPN_FEATUREMAPS//2)
        layer_outputs = [] 
        for p in rpn_feature_maps:
            layer_outputs.append(rpn([p]))
        output_names = ["rpn_class_logits", "rpn_class", "rpn_bbox"]
        outputs = list(zip(*layer_outputs))
        outputs = [KL.Concatenate(axis=1, name=n)(list(o)) for o, n in zip(outputs, output_names)]
        rpn_class_logits, rpn_class, rpn_bbox = outputs

        # Generate proposals [batch, N, (y1, x1, y2, x2)] in normalized coordinates (relative to img) where N = proposal count
        proposal_count = config.POST_NMS_ROIS_TRAINING if mode == "training"\
            else config.POST_NMS_ROIS_INFERENCE
        rpn_rois = mrcnn_model.ProposalLayer(
            proposal_count=proposal_count,
            nms_threshold=config.RPN_NMS_THRESHOLD,
            name="ROI",
            config=config)([rpn_class, rpn_bbox, anchors])


        if mode == "training":
            # Class ID mask to mark class IDs supported by the dataset the image came from
            active_class_ids = KL.Lambda(lambda x: mrcnn_model.parse_image_meta_graph(x)["active_class_ids"])(input_image_meta)

            if not config.USE_RPN_ROIS:
                # Ignore predicted ROIs and use ROIs provided as an input
                input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4], name="input_roi", dtype=np.int32)
                # Normalize coordinates
                target_rois = KL.Lambda(lambda x: mrcnn_model.norm_boxes_graph(x, K.shape(input_image)[1:3]))(input_rois)
            else:
                target_rois = rpn_rois

            # Generate detection targets
            # Subsamples proposals and generates target outputs for training
            # Note that proposal class IDs, gt_boxes, and gt_masks are zero
            # padded. Equally, returned rois and targets are zero padded.
  
            # CHANGE - Created a new target layer to transform GT grasps (gt_grasps) relative to proposal regions (target_grasps)
            rois, target_class_ids, target_bbox, target_mask, target_grasps =\
                GraspTargetLayer(config, name="proposal_targets")([
                    target_rois, input_gt_class_ids, gt_boxes, input_gt_masks, gt_grasps])

            # Network Heads
            # CHANGE - Modified fpn_classifier_graph by adding an extra head for grasp detection
            mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_grasps =\
                fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta,
                                     config.POOL_SIZE, num_classes=2,
                                     train_bn=config.TRAIN_BN,
                                     fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)
            if config.MODEL == 'mrcnn':
                mrcnn_mask = smrcnn_model.fpn_mask_graph(rois, mrcnn_feature_maps,
                                                  input_image_meta,
                                                  config.MASK_POOL_SIZE,
                                                  num_classes=2,
                                                  train_bn=config.TRAIN_BN)

            output_rois = KL.Lambda(lambda x: x * 1, name="output_rois")(rois)

            # Losses
            rpn_class_loss = KL.Lambda(lambda x: mrcnn_model.rpn_class_loss_graph(*x), name="rpn_class_loss")(
                [input_rpn_match, rpn_class_logits])
            rpn_bbox_loss = KL.Lambda(lambda x: mrcnn_model.rpn_bbox_loss_graph(config, *x), name="rpn_bbox_loss")(
                [input_rpn_bbox, input_rpn_match, rpn_bbox])
            # Use custom class loss without using active_class_ids
            class_loss = KL.Lambda(lambda x: smrcnn_model.mrcnn_class_loss_graph(*x), name="mrcnn_class_loss")(
                [target_class_ids, mrcnn_class_logits, active_class_ids])
            bbox_loss = KL.Lambda(lambda x: mrcnn_model.mrcnn_bbox_loss_graph(*x), name="mrcnn_bbox_loss")(
                [target_bbox, target_class_ids, mrcnn_bbox])
            # CHANGE: Added loss for grasp detection
            grasp_loss = KL.Lambda(lambda x: mrcnn_grasp_loss_graph(*x), name="mrcnn_grasp_loss")(
                [target_grasps, target_class_ids, mrcnn_grasps])
            if config.MODEL == 'mrcnn':
                mask_loss = KL.Lambda(lambda x: mrcnn_model.mrcnn_mask_loss_graph(*x), name="mrcnn_mask_loss")(
                    [target_mask, target_class_ids, mrcnn_mask])

            # Building the final Model
            # CHANGE: Added input layer for GT grasps (input_gt_grasps) and grasp detection loss (grasp_loss)
            # TODO - forgot to add grasps as output for training
            inputs = [input_image, input_image_meta, input_target,
                      input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks, input_gt_grasps]
            if not config.USE_RPN_ROIS:
                inputs.append(input_rois)
            if config.MODEL == 'mrcnn':
                outputs = [rpn_class_logits, rpn_class, rpn_bbox,
                           mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask,
                           rpn_rois, output_rois,
                           rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss, grasp_loss]
            elif config.MODEL =='frcnn':
                outputs = [rpn_class_logits, rpn_class, rpn_bbox,
                           mrcnn_class_logits, mrcnn_class, mrcnn_bbox,
                           rpn_rois, output_rois,
                           rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss]
            model = KM.Model(inputs, outputs, name='mask_rcnn')
        else: # if "inference"
            # Network Heads
            # Proposal classifier and BBox regressor heads
            # CHANGE: Get grasp predictions from new fpn_classifier_graph
            mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_grasps = \
                fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,
                                     config.POOL_SIZE, num_classes=2,
                                     train_bn=config.TRAIN_BN, 
                                     fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)

            # Detections
            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score, cx, cy, w, h, t)] in
            # normalized coordinates
            # CHANGE: Created new DetectionLayer() to include grasps
            detections = DetectionLayer(config, name="mrcnn_detection")(
                [rpn_rois, mrcnn_class, mrcnn_bbox, mrcnn_grasps, input_image_meta])

            # Create masks for detections
            detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections)
            if config.MODEL == 'mrcnn':
                mrcnn_mask = smrcnn_model.fpn_mask_graph(detection_boxes, mrcnn_feature_maps,
                                                  input_image_meta,
                                                  config.MASK_POOL_SIZE,
                                                  num_classes=2,
                                                  train_bn=config.TRAIN_BN)
            
            inputs = [input_image, input_image_meta, input_target, input_anchors]
            if config.MODEL == 'mrcnn':
                # CHANGE: Added grasps to output
                outputs = [detections, mrcnn_class, mrcnn_bbox,
                           mrcnn_mask, mrcnn_grasps, rpn_rois, rpn_class, rpn_bbox]
            elif config.MODEL =='frcnn':
                outputs = [detections, mrcnn_class, mrcnn_bbox,
                           rpn_rois, rpn_class, rpn_bbox]
            model = KM.Model(inputs, outputs, name='mask_rcnn')


        # Add multi-GPU support.
        if config.GPU_COUNT > 1:
            from mrcnn.parallel_model import ParallelModel
            model = ParallelModel(model, config.GPU_COUNT)

        return model


    def compile(self, learning_rate, momentum):
        """Gets the model ready for training. Adds losses, regularization, and
        metrics. Then calls the Keras compile() function.
        """
        # Optimizer object
        optimizer = keras.optimizers.SGD(
            lr=learning_rate, momentum=momentum,
            clipnorm=self.config.GRADIENT_CLIP_NORM)
        # Add Losses
        # First, clear previously set losses to avoid duplication
        # CHANGE: Include grasp loss
        self.keras_model._losses = []
        self.keras_model._per_input_losses = {}
        loss_names = [
            "rpn_class_loss",  "rpn_bbox_loss",
            "mrcnn_class_loss", "mrcnn_bbox_loss", "mrcnn_mask_loss", "mrcnn_grasp_loss"]
        for name in loss_names:
            layer = self.keras_model.get_layer(name)
            if layer.output in self.keras_model.losses:
                continue
            loss = (
                tf.reduce_mean(layer.output, keepdims=True)
                * self.config.LOSS_WEIGHTS.get(name, 1.))
            self.keras_model.add_loss(loss)

        # Add L2 Regularization
        # Skip gamma and beta weights of batch normalization layers.
        reg_losses = [
            keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32)
            for w in self.keras_model.trainable_weights
            if 'gamma' not in w.name and 'beta' not in w.name]
        self.keras_model.add_loss(tf.add_n(reg_losses))

        # Compile
        self.keras_model.compile(
            optimizer=optimizer,
            loss=[None] * len(self.keras_model.outputs))

        # Add metrics for losses
        for name in loss_names:
            if name in self.keras_model.metrics_names:
                continue
            layer = self.keras_model.get_layer(name)
            self.keras_model.metrics_names.append(name)
            loss = (
                tf.reduce_mean(layer.output, keepdims=True)
                * self.config.LOSS_WEIGHTS.get(name, 1.))
            self.keras_model.metrics_tensors.append(loss)

    
    def set_log_dir(self, model_path=None):
        """Sets the model log directory and epoch counter.
        model_path: If None, or a format different from what this code uses
            then set a new log directory and start epochs from 0. Otherwise,
            extract the log directory and the epoch counter from the file
            name.
        """
        # Set date and epoch counter as if starting a new model
        self.epoch = 0

        # Directory for training logs
        self.log_dir = os.path.join(self.model_dir, 
                                    "gs{}_{}_{}".format(self.config.MODEL.lower(), 
                                                              self.config.NAME.lower(), 
                                                              self.config.EXPERIMENT.lower()))

        # Create log_dir if not exists
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)

        # Path to save after each epoch. Include placeholders that get filled by Keras.
        self.checkpoint_path = os.path.join(self.log_dir, "gsmrcnn_*epoch*.h5")
        self.checkpoint_path = self.checkpoint_path.replace("*epoch*", "{epoch:04d}")
    

    def train(self, train_dataset, val_dataset, learning_rate, epochs, layers,
              augmentation=None):
        """Train the model. 
        train_dataset, val_dataset: Training and validation Dataset objects.
        learning_rate: The learning rate to train with
        epochs: Number of training epochs. Note that previous training epochs
                are considered to be done alreay, so this actually determines
                the epochs to train in total rather than in this particaular
                call.
        layers: Allows selecting wich layers to train. It can be:
            - A regular expression to match layer names to train
            - One of these predefined values:
              heaads: The RPN, classifier and mask heads of the network
              all: All the layers
              3+: Train Resnet stage 3 and up
              4+: Train Resnet stage 4 and up
              5+: Train Resnet stage 5 and up
        augmentation: Augmentation not supported for GSMR-CNN.
        """
        assert self.mode == "training", "Create model in training mode."

        # Pre-defined layer regular expressions
        layer_regex = {
            # all layers but the backbone
            "heads": r"(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)",
            # region proposal network
            "rpn": r"(rpn\_.*)|(fpn\_.*)",
            # From a specific Resnet stage and up
            "2+": r"(res2.*)|(bn2.*)|(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)",
            "3+": r"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)",
            "4+": r"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)",
            "5+": r"(res5.*)|(bn5.*)|(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)",
            # All layers
            "all": ".*",
        }
        if layers in layer_regex.keys():
            layers = layer_regex[layers]

        # Data generators
        # CHANGE: Use grasp data generators
        train_generator = grasp_data_generator(train_dataset, self.config, shuffle=True,
                                         augmentation=augmentation,
                                         batch_size=self.config.BATCH_SIZE)
        val_generator = grasp_data_generator(val_dataset, self.config, shuffle=True,
                                       batch_size=self.config.BATCH_SIZE)

        # Callbacks
        callbacks = [
            keras.callbacks.TensorBoard(log_dir=self.log_dir,
                                        histogram_freq=0, write_graph=True, write_images=False),
            keras.callbacks.ModelCheckpoint(self.checkpoint_path,
                                            verbose=0, save_weights_only=True),
        ]

        # Train
        mrcnn_model.log("\nStarting at epoch {}. LR={}\n".format(self.epoch, learning_rate))
        mrcnn_model.log("Checkpoint Path: {}".format(self.checkpoint_path))
        self.set_trainable(layers)
        self.compile(learning_rate, self.config.LEARNING_MOMENTUM)

        # Work-around for Windows: Keras fails on Windows when using
        # multiprocessing workers. See discussion here:
        # https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009
        if os.name is 'nt':
            workers = 0
        else:
            workers = multiprocessing.cpu_count()

        self.keras_model.fit_generator(
            train_generator,
            initial_epoch=self.epoch,
            epochs=epochs,
            steps_per_epoch=self.config.STEPS_PER_EPOCH,
            callbacks=callbacks,
            validation_data=val_generator,
            validation_steps=self.config.VALIDATION_STEPS,
            max_queue_size=100,
            workers=workers,
            use_multiprocessing=True,
        )
        self.epoch = max(self.epoch, epochs)

    def unmold_detections(self, detections, mrcnn_mask, original_image_shape,
                          image_shape, window):
        """Reformats the detections of one image from the format of the neural
        network output to a format suitable for use in the rest of the
        application. Transforms bounding boxes and grasps from normalized image 
        co-ordinates to pixel co-ordinates (i.e. transforms predictions).

        detections: [N, (y1, x1, y2, x2, class_id, score, cx, cy, w, h, t)] in normalized coordinates
        mrcnn_mask: [N, height, width, num_classes]
        original_image_shape: [H, W, C] Original image shape before resizing
        image_shape: [H, W, C] Shape of the image after resizing and padding
        window: [y1, x1, y2, x2] Pixel coordinates of box in the image where the real
                image is excluding the padding.

        Returns:
        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels
        class_ids: [N] Integer class IDs for each bounding box
        scores: [N] Float probability scores of the class_id
        masks: [height, width, num_instances] Instance masks
        grasps: [N, (cx, cy, w, h, t)] Grasps in pixels
        """
        # How many detections do we have?
        # Detections array is padded with zeros. Find the first class_id == 0.
        zero_ix = np.where(detections[:, 4] == 0)[0]
        N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]

        # Extract boxes, class_ids, scores, and class-specific masks
        boxes = detections[:N, :4]
        class_ids = detections[:N, 4].astype(np.int32)
        scores = detections[:N, 5]
        masks = mrcnn_mask[np.arange(N), :, :, class_ids]
        grasps = detections[:N, 6:] # CHANGE: Extract grasps

        # Translate normalized coordinates in the resized image to pixel
        # coordinates in the original image before resizing
        window = mrcnn_utils.norm_boxes(window, image_shape[:2])
        wy1, wx1, wy2, wx2 = window
        shift = np.array([wy1, wx1, wy1, wx1])
        wh = wy2 - wy1  # window height
        ww = wx2 - wx1  # window width
        scale = np.array([wh, ww, wh, ww])
        # Convert boxes to normalized coordinates on the window
        boxes = np.divide(boxes - shift, scale)
        # Convert boxes to pixel coordinates on the original image
        boxes = denorm_boxes(boxes, original_image_shape[:2])
        # CHANGE: convert normalized grasps (in normalized image co-ordinates) to pixel co-orindates
        grasps = np.divide(grasps - np.array([wx1, wy1, 0, 0, 0]), np.array([ww, wh, ww, wh, ww/ww]))
        grasps = denorm_grasps(grasps, original_image_shape[:2])

        # Filter out detections with zero area. Happens in early training when
        # network weights are still random
        exclude_ix = np.where(
            (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]
        if exclude_ix.shape[0] > 0:
            boxes = np.delete(boxes, exclude_ix, axis=0)
            class_ids = np.delete(class_ids, exclude_ix, axis=0)
            scores = np.delete(scores, exclude_ix, axis=0)
            masks = np.delete(masks, exclude_ix, axis=0)
            grasps = np.delete(grasps, exclude_ix, axis=0) # CHANGE: Filter out grasps as well
            N = class_ids.shape[0]

        # Resize masks to original image size and set boundary threshold.
        full_masks = []
        for i in range(N):
            # Convert neural network mask to full size mask
            full_mask = mrcnn_utils.unmold_mask(masks[i], boxes[i], original_image_shape)
            full_masks.append(full_mask)
        full_masks = np.stack(full_masks, axis=-1)\
            if full_masks else np.empty(original_image_shape[:2] + (0,))

        # CHANGE: Returns transformed grasps
        return boxes, class_ids, scores, full_masks, grasps

    def detect(self, targets, images, verbose=0, random_detections=False, eps=1e-6):
        """Runs the detection pipeline.

        images: List of images, potentially of different sizes.

        Returns a list of dicts, one dict per image. The dict contains:
        rois: [N, (y1, x1, y2, x2)] detection bounding boxes
        class_ids: [N] int class IDs
        scores: [N] float probability scores for the class IDs
        masks: [H, W, N] instance binary masks
        grasps: [N, (cx, cy, w, h, t)] float grasp co-ordinates
        """
        assert self.mode == "inference", "Create model in inference mode."
        assert len(
            images) == self.config.BATCH_SIZE, "len(images) must be equal to BATCH_SIZE"

        if verbose:
            mrcnn_model.log("Processing {} images".format(len(images)))
            for image in images:
                mrcnn_model.log("image", image)
                mrcnn_model.log("target", np.stack(targets))

        # Mold inputs to format expected by the neural network
        molded_images, image_metas, windows = self.mold_inputs(images)
        molded_targets = np.stack(targets)

        # Validate image sizes
        # All images in a batch MUST be of the same size
        image_shape = molded_images[0].shape
        for g in molded_images[1:]:
            assert g.shape == image_shape,\
                "After resizing, all images must have the same size. Check IMAGE_RESIZE_MODE and image sizes."
        # Add size assertion for target
        target_shape = molded_targets[0].shape
        for g in molded_targets[1:]:
            assert g.shape == target_shape,\
                "After resizing, all images must have the same size. Check IMAGE_RESIZE_MODE and image sizes."

        # Anchors
        anchors = self.get_anchors(image_shape)
        # Duplicate across the batch dimension because Keras requires it
        # TODO: can this be optimized to avoid duplicating the anchors?
        anchors = np.broadcast_to(anchors, (self.config.BATCH_SIZE,) + anchors.shape)

        if verbose:
            mrcnn_model.log("molded_images", molded_images)
            # modellib.log("image_metas", image_metas)
            mrcnn_model.log("molded_targets", molded_targets)
            # modellib.log("target_metas", target_metas)
            mrcnn_model.log("anchors", anchors)
        # Run object detection
        # CHANGE: Use GSM-RCNN model
        detections, _, _, mrcnn_mask, mrcnn_grasp, _, _, _ =\
            self.keras_model.predict([molded_images, image_metas, molded_targets, anchors], verbose=0)

        # Process detections
        results = []
        for i, image in enumerate(images):
            final_rois, final_class_ids, final_scores, final_masks, final_grasps =\
                self.unmold_detections(detections[i], mrcnn_mask[i],
                                       image.shape, molded_images[i].shape,
                                       windows[i])
            results.append({
                "rois": final_rois,
                "class_ids": final_class_ids,
                "scores": final_scores,
                "masks": final_masks,
                "grasps": final_grasps
            })
        return results


############################################################
#  Grasp Loss
############################################################

def mrcnn_grasp_loss_graph(target_grasp, target_class_ids, pred_grasp):
    """Smooth L1 Loss for refining grasp predictions.
    target_grasp: [batch, num_rois, max_gt_grasp, (x, y, w, h, t)] normalized wrt region
    target_class_ids: [batch, num_rois]. Integer class IDs.
    pred_grasp: [batch, num_rois, num_classes, (x, y, w, h, t)]
    """
    # Reshape to merge batch and roi dimensions for simplicity.
    target_class_ids = K.reshape(target_class_ids, (-1,))
    target_grasp = K.reshape(target_grasp, (-1, K.int_shape(target_grasp)[2], 5))
    pred_grasp = K.reshape(pred_grasp, (-1, K.int_shape(pred_grasp)[2], 5))

    # Only positive ROIs contribute to the loss. And only
    # the right class_id of each ROI. Get their indices.
    positive_roi_ix = tf.where(target_class_ids > 0)[:, 0]
    positive_roi_class_ids = tf.cast(
        tf.gather(target_class_ids, positive_roi_ix), tf.int64)
    indices = tf.stack([positive_roi_ix, positive_roi_class_ids], axis=1)

    # Gather the target_grasp
    target_grasp = tf.gather(target_grasp, positive_roi_ix, axis=0)
    pred_grasp = tf.gather_nd(pred_grasp, indices)

    # ------------------ CHOOSE THE NEAREST GT GRASP FOR EACH PRED GRASP -----------------------
    stack_pred = tf.reshape(tf.tile(tf.expand_dims(pred_grasp, 1), [1, 1, K.int_shape(target_grasp)[1]]),
                            (-1, K.int_shape(target_grasp)[1], K.int_shape(target_grasp)[2]))
    # Compute the score and take the smnallest for each index...
    gx, gy, _, _, gt = tf.split(target_grasp, 5, axis=2)
    px, py, _, _, pt = tf.split(stack_pred, 5, axis=2)
    # Calculate the differences (centre-coordinate and rotation) between the ground truths and predictions
    d = tf.abs(gx - px) + tf.abs(gy - py) + tf.abs(((gt - pt) + (90*np.pi/180))%(180*np.pi/180)-(90*np.pi/180))
    # Get the idxs of GTs with the smallest difference for each prediction
    min_idxs = tf.argmin(d, axis=1)
    chosen_target_grasp = tf.gather_nd(target_grasp, min_idxs, batch_dims=1) # select the closest ground-truth

    # Smooth-L1 Loss
    loss = K.switch(tf.size(target_grasp) > 0,
                    mrcnn_model.smooth_l1_loss(y_true=chosen_target_grasp, y_pred=pred_grasp),
                    tf.constant(0.0))
    loss = K.mean(loss)
    return loss


############################################################
#  Detection Target Layer
############################################################

def detection_targets_graph(proposals, gt_class_ids, gt_boxes, gt_masks, gt_grasps, config):
    """Generates detection targets for one image. Subsamples proposals and
    generates target class IDs, bounding box deltas, and masks for each.

    Inputs:
    proposals: [POST_NMS_ROIS_TRAINING, (y1, x1, y2, x2)] in normalized coordinates. Might
               be zero padded if there are not enough proposals.
    gt_class_ids: [MAX_GT_INSTANCES] int class IDs
    gt_boxes: [MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized coordinates.
    gt_masks: [height, width, MAX_GT_INSTANCES] of boolean type.
    gt_grasps: [MAX_GT_INSTANCES, MAX_GT_GRASPS, (cx, cy, w, h, t)] in image normalized co-ordinates.

    Returns: Target ROIs and corresponding class IDs, bounding box shifts,
    and masks.
    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized coordinates
    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs. Zero padded.
    deltas: [TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw))]
    masks: [TRAIN_ROIS_PER_IMAGE, height, width]. Masks cropped to bbox
           boundaries and resized to neural network output size.
    grasps: [TRAIN_ROIS_PER_IMAGE, MAX_GT_GRASPS, (cx, cy, w, h, t)]

    Note: Returned arrays might be zero padded if not enough target ROIs.
    """
    # Assertions
    asserts = [
        tf.Assert(tf.greater(tf.shape(proposals)[0], 0), [proposals],
                  name="roi_assertion"),
    ]
    with tf.control_dependencies(asserts):
        proposals = tf.identity(proposals)

    # Remove zero padding
    proposals, _ = mrcnn_model.trim_zeros_graph(proposals, name="trim_proposals")
    gt_boxes, non_zeros =  mrcnn_model.trim_zeros_graph(gt_boxes, name="trim_gt_boxes")
    gt_class_ids = tf.boolean_mask(gt_class_ids, non_zeros,
                                   name="trim_gt_class_ids")
    gt_masks = tf.gather(gt_masks, tf.where(non_zeros)[:, 0], axis=2,
                         name="trim_gt_masks")
    # CHANGE: Remove zero padding from gt_grasps
    gt_grasps = tf.gather(gt_grasps, tf.where(non_zeros)[:, 0], axis=0, name="trim_gt_grasps") # quick fix - need to ensure non-zero grasps are not passed through generator

    # Handle COCO crowds
    # A crowd box in COCO is a bounding box around several instances. Exclude
    # them from training. A crowd box is given a negative class ID.
    crowd_ix = tf.where(gt_class_ids < 0)[:, 0]
    non_crowd_ix = tf.where(gt_class_ids > 0)[:, 0]
    crowd_boxes = tf.gather(gt_boxes, crowd_ix)
    gt_class_ids = tf.gather(gt_class_ids, non_crowd_ix)
    gt_boxes = tf.gather(gt_boxes, non_crowd_ix)
    gt_masks = tf.gather(gt_masks, non_crowd_ix, axis=2)
    gt_grasps = tf.gather(gt_grasps, non_crowd_ix, axis=0) # CHANGE: Filter grasps

    # Compute overlaps matrix [proposals, gt_boxes] - refers to idxs
    overlaps = mrcnn_model.overlaps_graph(proposals, gt_boxes) 

    # Compute overlaps with crowd boxes [proposals, crowd_boxes]
    crowd_overlaps = mrcnn_model.overlaps_graph(proposals, crowd_boxes)
    crowd_iou_max = tf.reduce_max(crowd_overlaps, axis=1)
    no_crowd_bool = (crowd_iou_max < 0.001)

    # Determine positive and negative ROIs
    roi_iou_max = tf.reduce_max(overlaps, axis=1) # gets the max overlap score for each proposal idx (size proposal)
    # 1. Positive ROIs are those with >= 0.5 IoU with a GT box
    positive_roi_bool = (roi_iou_max >= 0.5) # creates a bool area of above
    positive_indices = tf.where(positive_roi_bool)[:, 0] # list of indices of (positive roi bool)
    # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.
    negative_indices = tf.where(tf.logical_and(roi_iou_max < 0.5, no_crowd_bool))[:, 0]

    # Subsample ROIs. Aim for 33% positive
    # Positive ROIs
    positive_count = int(config.TRAIN_ROIS_PER_IMAGE *
                         config.ROI_POSITIVE_RATIO)
    positive_indices = tf.random_shuffle(positive_indices)[:positive_count] # randomly sample up to X of the positive idxs
    positive_count = tf.shape(positive_indices)[0] # get number of positives
    # Negative ROIs. Add enough to maintain positive:negative ratio.
    r = 1.0 / config.ROI_POSITIVE_RATIO
    negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count
    negative_indices = tf.random_shuffle(negative_indices)[:negative_count]
    # Gather selected ROIs - basically splits proposals into positive and negatives
    positive_rois = tf.gather(proposals, positive_indices)
    negative_rois = tf.gather(proposals, negative_indices)

    # Assign positive ROIs to GT boxes.
    positive_overlaps = tf.gather(overlaps, positive_indices)
    roi_gt_box_assignment = tf.cond(  # get the idxs of the gt boxes
        tf.greater(tf.shape(positive_overlaps)[1], 0),
        true_fn = lambda: tf.argmax(positive_overlaps, axis=1),
        false_fn = lambda: tf.cast(tf.constant([]), tf.int64)
    )
    roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment)
    roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment)
    # CHANGE: Filter grasps by positive ROIs [NUM_POSITIVE_PROPOSALS, MAX_GT_GRASPS, 5]
    roi_gt_grasps = tf.gather(gt_grasps, roi_gt_box_assignment, axis=0)  # [NUM_POSITIVE_PROPOSALS, MAX_GT_GRASPS, 5]

    # Compute bbox refinement for positive ROIs
    deltas = mrcnn_utils.box_refinement_graph(positive_rois, roi_gt_boxes)
    deltas /= config.BBOX_STD_DEV

    # Assign positive ROIs to GT masks
    # Permute masks to [N, height, width, 1]
    transposed_masks = tf.expand_dims(tf.transpose(gt_masks, [2, 0, 1]), -1)
    # Pick the right mask for each ROI
    roi_masks = tf.gather(transposed_masks, roi_gt_box_assignment)

    # Compute mask targets
    boxes = positive_rois
    # CHANGE: Copy positive grasps, and stack each positive ROI to a positive grasp
    transformed_gt_grasps = roi_gt_grasps
    stacked_rois = tf.tile(tf.expand_dims(positive_rois, 1), [1, tf.shape(gt_grasps)[1], 1])
    if config.USE_MINI_MASK:
        # Transform ROI coordinates from normalized image space
        # to normalized mini-mask space.
        y1, x1, y2, x2 = tf.split(positive_rois, 4, axis=1) #N
        gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(roi_gt_boxes, 4, axis=1) #N
        gt_h = gt_y2 - gt_y1
        gt_w = gt_x2 - gt_x1
        y1 = (y1 - gt_y1) / gt_h
        x1 = (x1 - gt_x1) / gt_w
        y2 = (y2 - gt_y1) / gt_h
        x2 = (x2 - gt_x1) / gt_w
        boxes = tf.concat([y1, x1, y2, x2], 1)

    # CHANGE: Crop and resize grasps according to shape of ROIs
    gx, gy, gw, gh, gt = tf.split(transformed_gt_grasps, 5, axis=2)
    ry1, rx1, ry2, rx2 = tf.split(stacked_rois, 4, axis=2)
    w = rx2 - rx1
    h = ry2 - ry1
    nx = (gx - rx1) * (1 / w)
    ny = (gy - ry1) * (1 / h)
    nw = gw * (1 / w)
    nh = gh * (1 / h)
    transformed_gt_grasps = tf.concat([nx, ny, nw, nh, gt], 2)

    box_ids = tf.range(0, tf.shape(roi_masks)[0])
    masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes, # (x1, y1, x2, y2)
                                     box_ids,
                                     config.MASK_SHAPE) #(x32 32)
    # Remove the extra dimension from masks.
    masks = tf.squeeze(masks, axis=3)

    # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with
    # binary cross entropy loss.
    masks = tf.round(masks)

    # Append negative ROIs and pad bbox deltas and masks that
    # are not used for negative ROIs with zeros.
    rois = tf.concat([positive_rois, negative_rois], axis=0)
    N = tf.shape(negative_rois)[0]
    P = tf.maximum(config.TRAIN_ROIS_PER_IMAGE - tf.shape(rois)[0], 0)
    rois = tf.pad(rois, [(0, P), (0, 0)])
    roi_gt_boxes = tf.pad(roi_gt_boxes, [(0, N + P), (0, 0)])
    roi_gt_class_ids = tf.pad(roi_gt_class_ids, [(0, N + P)])
    deltas = tf.pad(deltas, [(0, N + P), (0, 0)])
    masks = tf.pad(masks, [[0, N + P], (0, 0), (0, 0)])
    # CHANGE: Do the same for grasps.
    final_grasps = tf.pad(transformed_gt_grasps, [(0, N + P), (0, 0), (0, 0)])

    return rois, roi_gt_class_ids, deltas, masks, final_grasps

# CHANGE: Added grasps to Target Layer
class GraspTargetLayer(KE.Layer):
    """Subsamples proposals and generates target box refinement, class_ids,
    and masks for each.

    Inputs:
    proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might
               be zero padded if there are not enough proposals.
    gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.
    gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized
              coordinates.
    gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type
    gt_grasps: [batch, MAX_GT_INSTANCES, MAX_GT_GRASPS, (cx, cy, w, h, t)] in normalized co-ordinates.

    Returns: Target ROIs and corresponding class IDs, bounding box shifts,
    and masks.
    rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized
          coordinates
    target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.
    target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw)]
    target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width]
                 Masks cropped to bbox boundaries and resized to neural
                 network output size.
    target_grasps: [batch, TRAIN_ROIS_PER_IMAGE, MAX_GT_GRASPS, (cx, cy, w, h, t)]

    Note: Returned arrays might be zero padded if not enough target ROIs.
    """

    def __init__(self, config, **kwargs):
        super(GraspTargetLayer, self).__init__(**kwargs)
        self.config = config

    def call(self, inputs):
        proposals = inputs[0]
        gt_class_ids = inputs[1]
        gt_boxes = inputs[2]
        gt_masks = inputs[3]
        gt_grasps = inputs[4]

        # Slice the batch and run a graph for each slice
        # TODO: Rename target_bbox to target_deltas for clarity
        names = ["rois", "target_class_ids", "target_bbox", "target_mask", "target_grasps"]
        outputs = mrcnn_utils.batch_slice(
            [proposals, gt_class_ids, gt_boxes, gt_masks, gt_grasps],
            lambda w, x, y, z, g: detection_targets_graph(
                w, x, y, z, g, self.config),
            self.config.IMAGES_PER_GPU, names=names)
        return outputs

    def compute_output_shape(self, input_shape):
        return [
            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # rois
            (None, self.config.TRAIN_ROIS_PER_IMAGE),  # class_ids
            (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # deltas
            (None, self.config.TRAIN_ROIS_PER_IMAGE, self.config.MASK_SHAPE[0],
             self.config.MASK_SHAPE[1]) , # masks
            (None, self.config.TRAIN_ROIS_PER_IMAGE, self.config.MAX_GT_GRASPS, 5) # grasps
        ]

    def compute_mask(self, inputs, mask=None):
        return [None, None, None, None, None]


############################################################
#  Miscellenous Graph Functions
############################################################
def norm_grasps_graph(grasps, shape):
    """Converts grasps from pixel coordinates to normalized coordinates.
    grasps: [..., (x, y, w, h, t)] in pixel coordinates
    shape: [..., (height, width)] (e.g. img or mask) in pixels

    Returns:
        [..., (x, y, w, h, t)] in normalized coordinates
    """
    h, w = tf.split(tf.cast(shape, tf.float32), 2)
    scale = tf.concat([w, h, w, h, w/w], axis=-1) # last idx is 1.0 because we don't want to change it...
    return tf.divide(grasps, scale)


def denorm_boxes(boxes, shape):
    """Converts boxes from normalized coordinates to pixel coordinates.
    boxes: [N, (y1, x1, y2, x2)] in normalized coordinates
    shape: [..., (height, width)] in pixels

    Note: In pixel coordinates (y2, x2) is outside the box. But in normalized
    coordinates it's inside the box.

    Returns:
        [N, (y1, x1, y2, x2)] in pixel coordinates
    """
    h, w = shape
    scale = np.array([h - 1, w - 1, h - 1, w - 1])
    shift = np.array([0, 0, 1, 1])
    return np.around(np.multiply(boxes, scale) + shift).astype(np.int32)

def denorm_grasps(grasps, shape):
    """Converts grasps from pixel coordinates to normalized coordinates.
    grasps: [..., (x, y, w, h, t)] in pixel coordinates
    shape: [..., (height, width)] (e.g. img or mask) in pixels

    Returns:
        [..., (x, y, w, h, t)] in normalized coordinates
    """
    h, w = shape
    scale = np.array([w, h, w, h, 1.0]) # last idx is 1.0 because we don't want to change it...
    return np.around(np.multiply(grasps, scale), 5)


def fpn_classifier_graph(rois, feature_maps, image_meta,
                         pool_size, num_classes, train_bn=True, fc_layers_size=1024):
    """Builds the computation graph of the feature pyramid network classifier
    and regressor heads.

    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized
          coordinates.
    feature_maps: List of feature maps from diffent layers of the pyramid,
                  [P2, P3, P4, P5]. Each has a different resolution.
    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()
    pool_size: The width of the square feature map generated from ROI Pooling.
    num_classes: number of classes, which determines the depth of the results
    train_bn: Boolean. Train or freeze Batch Norm layers

    Returns:
        logits: [N, NUM_CLASSES] classifier logits (before softmax)
        probs: [N, NUM_CLASSES] classifier probabilities
        bbox_deltas: [N, (dy, dx, log(dh), log(dw))] Deltas to apply to
                     proposal boxes
        grasps: [N, (cx, cy, w, h, t)]
    """
    # ROI Pooling
    # Shape: [batch, num_boxes, pool_height, pool_width, channels]
    px = mrcnn_model.PyramidROIAlign([pool_size, pool_size],
                        name="roi_align_classifier")([rois, image_meta] + feature_maps)
    # Two 1024 FC layers (implemented with Conv2D for consistency)
    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding="valid"),
                           name="mrcnn_class_conv1")(px)
    x = KL.TimeDistributed(mrcnn_model.BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)
    x = KL.Activation('relu')(x)
    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),
                           name="mrcnn_class_conv2")(x)
    x = KL.TimeDistributed(mrcnn_model.BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)
    x = KL.Activation('relu')(x)

    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),
                       name="pool_squeeze")(x)

    # Another Two 1024 FC layers (for grasp detection)
    # Two 1024 FC layers (implemented with Conv2D for consistency)
    gx = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding="valid"),
                           name="mrcnn_grasp_conv1")(px)
    gx = KL.TimeDistributed(mrcnn_model.BatchNorm(), name='mrcnn_grasp_bn1')(gx, training=train_bn)
    gx = KL.Activation('relu')(gx)
    gx = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),
                           name="mrcnn_grasp_conv2")(gx)
    gx = KL.TimeDistributed(mrcnn_model.BatchNorm(), name='mrcnn_grasp_bn2')(gx, training=train_bn)
    gx = KL.Activation('relu')(gx)

    gx_shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),
                       name="mrcnn_grasp_pool_squeeze")(gx)


    # Classifier head
    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),
                                            name='mrcnn_class_logits')(shared)
    mrcnn_probs = KL.TimeDistributed(KL.Activation("softmax"),
                                     name="mrcnn_class")(mrcnn_class_logits)

    # CHANGE: Grasp prediction head
    # [batch, grasps, num_classes, (cx, cy, w, h, t)]
    grasp = KL.TimeDistributed(KL.Dense(5, activation='linear'),
                           name='mrcnn_grasp_box_fc')(gx_shared)
    g_shape = K.int_shape(grasp)
    grasp = KL.Reshape((g_shape[1], 1, 5), name="mrcnn_grasp_box")(grasp)
    grasp_box = KL.Concatenate(axis=-2)([grasp for i in range(num_classes)])

    # BBox head
    # [batch, boxes, num_classes * (dy, dx, log(dh), log(dw))]
    x = KL.TimeDistributed(KL.Dense(4, activation='linear'),
                           name='mrcnn_bbox_fc')(shared)
    # Reshape to [batch, boxes, num_classes, (dy, dx, log(dh), log(dw))]
    s = K.int_shape(x)
    x = KL.Reshape((s[1],1, 4), name="mrcnn_bbox")(x)
    # Duplicate output for fg/bg detections
    mrcnn_bbox = KL.Concatenate(axis=-2)([x for i in range(num_classes)])

    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox, grasp_box

# CHANGE: Added grasps to Detection Layer
class DetectionLayer(KE.Layer):
    """Takes classified proposal boxes and their bounding box deltas and
    returns the final detection boxes.

    Returns:
    [batch, num_detections, (y1, x1, y2, x2, class_id, class_score , x1, y1, w, h, t)] where
    coordinates are normalized.
    """

    def __init__(self, config=None, **kwargs):
        super(DetectionLayer, self).__init__(**kwargs)
        self.config = config

    def call(self, inputs):
        rois = inputs[0]
        mrcnn_class = inputs[1]
        mrcnn_bbox = inputs[2]
        mrcnn_grasp = inputs[3]
        image_meta = inputs[4]

        # Get windows of images in normalized coordinates. Windows are the area
        # in the image that excludes the padding.
        # Use the shape of the first image in the batch to normalize the window
        # because we know that all images get resized to the same size.
        m = mrcnn_model.parse_image_meta_graph(image_meta)
        image_shape = m['image_shape'][0]
        window = mrcnn_model.norm_boxes_graph(m['window'], image_shape[:2])

        # Run detection refinement graph on each item in the batch
        detections_batch = mrcnn_utils.batch_slice(
            [rois, mrcnn_class, mrcnn_bbox, mrcnn_grasp, window],
            lambda x, y, w, g, z: refine_detections_graph(x, y, w, g, z, self.config),
            self.config.IMAGES_PER_GPU)

        # Reshape output
        # [batch, num_detections, (y1, x1, y2, x2, class_id, class_score, x1, y1, w, h, t)] in
        # normalized coordinates
        return tf.reshape(
            detections_batch,
            [self.config.BATCH_SIZE, self.config.DETECTION_MAX_INSTANCES, 11])

    def compute_output_shape(self, input_shape):
        return (None, self.config.DETECTION_MAX_INSTANCES, 11)

def refine_detections_graph(rois, probs, deltas, grasps, window, config):
    """Refine classified proposals and filter overlaps and return final
    detections.

    Inputs:
        rois: [N, (y1, x1, y2, x2)] in normalized coordinates
        probs: [N, num_classes]. Class probabilities.
        grasps: [N, num_classes, (x, y, w, h, t)]
        deltas: [N, num_classes, (dy, dx, log(dh), log(dw))]. Class-specific
                bounding box deltas.
        window: (y1, x1, y2, x2) in normalized coordinates. The part of the image
            that contains the image excluding the padding.

    Returns detections shaped: [num_detections, (y1, x1, y2, x2, class_id, score)] where
        coordinates are normalized.
    """
    # CHANGE: Convert grasps back from normalized to image co-ordinates
    stacked_rois = tf.tile(tf.expand_dims(rois, 1), [1, config.NUM_CLASSES, 1])
    gx, gy, gw, gh, gt = tf.split(grasps, 5, axis=2)
    ry1, rx1, ry2, rx2 = tf.split(stacked_rois, 4, axis=2)
    w = rx2 - rx1
    h = ry2 - ry1
    nx = (gx * w) + rx1
    ny = (gy * h) + ry1
    nw = gw * w
    nh = gh * h
    transformed_grasps = tf.concat([nx, ny, nw, nh, gt], 2)

    # Class IDs per ROI
    class_ids = tf.argmax(probs, axis=1, output_type=tf.int32)
    # Class probability of the top class of each ROI
    indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)
    class_scores = tf.gather_nd(probs, indices)
    # Class-specific bounding box deltas
    deltas_specific = tf.gather_nd(deltas, indices)
    # CHANGE: Filter grasps
    gathered_grasps = tf.gather_nd(transformed_grasps, indices)
    # Apply bounding box deltas
    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates
    refined_rois = mrcnn_model.apply_box_deltas_graph(
        rois, deltas_specific * config.BBOX_STD_DEV)
    # Clip boxes to image window
    refined_rois = mrcnn_model.clip_boxes_graph(refined_rois, window)

    # TODO: Filter out boxes with zero area

    # Filter out background boxes
    keep = tf.where(class_ids > 0)[:, 0]
    # Filter out low confidence boxes
    if config.DETECTION_MIN_CONFIDENCE:
        conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]
        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),
                                        tf.expand_dims(conf_keep, 0))
        keep = tf.sparse_tensor_to_dense(keep)[0]

    # Apply per-class NMS
    # 1. Prepare variables
    pre_nms_class_ids = tf.gather(class_ids, keep)
    pre_nms_scores = tf.gather(class_scores, keep)
    pre_nms_rois = tf.gather(refined_rois, keep)
    unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]

    def nms_keep_map(class_id):
        """Apply Non-Maximum Suppression on ROIs of the given class."""
        # Indices of ROIs of the given class
        ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]
        # Apply NMS
        class_keep = tf.image.non_max_suppression(
            tf.gather(pre_nms_rois, ixs),
            tf.gather(pre_nms_scores, ixs),
            max_output_size=config.DETECTION_MAX_INSTANCES,
            iou_threshold=config.DETECTION_NMS_THRESHOLD)
        # Map indices
        class_keep = tf.gather(keep, tf.gather(ixs, class_keep))
        # Pad with -1 so returned tensors have the same shape
        gap = config.DETECTION_MAX_INSTANCES - tf.shape(class_keep)[0]
        class_keep = tf.pad(class_keep, [(0, gap)],
                            mode='CONSTANT', constant_values=-1)
        # Set shape so map_fn() can infer result shape
        class_keep.set_shape([config.DETECTION_MAX_INSTANCES])
        return class_keep

    # 2. Map over class IDs
    nms_keep = tf.map_fn(nms_keep_map, unique_pre_nms_class_ids,
                         dtype=tf.int64)
    # 3. Merge results into one list, and remove -1 padding
    nms_keep = tf.reshape(nms_keep, [-1])
    nms_keep = tf.gather(nms_keep, tf.where(nms_keep > -1)[:, 0])
    # 4. Compute intersection between keep and nms_keep
    keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),
                                    tf.expand_dims(nms_keep, 0))
    keep = tf.sparse_tensor_to_dense(keep)[0]
    # Keep top detections
    roi_count = config.DETECTION_MAX_INSTANCES
    class_scores_keep = tf.gather(class_scores, keep)
    num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)
    top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]
    keep = tf.gather(keep, top_ids)

    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]
    # Coordinates are normalized.
    # CHANGE: Added grasps
    detections = tf.concat([
        tf.gather(refined_rois, keep),
        tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],
        tf.gather(class_scores, keep)[..., tf.newaxis],
        tf.gather(gathered_grasps, keep)
    ], axis=1)

    # Pad with zeros if detections < DETECTION_MAX_INSTANCES
    gap = config.DETECTION_MAX_INSTANCES - tf.shape(detections)[0]
    detections = tf.pad(detections, [(0, gap), (0, 0)], "CONSTANT")
    return detections

############################################################
#  Other Graph Functions
############################################################
def resize_grasps(grasps, padding):
    """ Adding padding to grasp and return. """
    i = np.nonzero(grasps) # get indices of nonzero elements
    grasps[i[0], i[1], 1] += padding[0][0] # shift non-zero y values
    grasps[i[0], i[1], 0] += padding[1][0] # shift non-zero x values
    return grasps